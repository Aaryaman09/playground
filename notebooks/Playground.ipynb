{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the library to the Python path so that we can import its modules\n",
    "LIB_DIR = os.path.abspath(os.path.join(\"..\", \"games\"))\n",
    "\n",
    "if not LIB_DIR in sys.path:\n",
    "    sys.path.append(LIB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from a.agents import RandomAgent\n",
    "from a.pommerman.configs import pommerman_ffa_v0\n",
    "from a.pommerman.envs.v0 import Pomme\n",
    "from a.pommerman.characters import Agent\n",
    "import a.utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random agents\n",
    "\n",
    "The following codes instantiates the environment with four random agents who take actions until the game is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = pommerman_ffa_v0()\n",
    "env = Pomme(**config[\"env_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add four random agents\n",
    "agents = {}\n",
    "for agent_id in range(4):\n",
    "    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "env.set_agents(list(agents.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This board has unreachable passages or agents. Re-making...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cinjon/Code/playground/venv/lib/python3.6/site-packages/scipy/misc/pilutil.py:480: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Users/cinjon/Code/playground/venv/lib/python3.6/site-packages/scipy/misc/pilutil.py:483: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    }
   ],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the random agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(obs)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Agents\n",
    "\n",
    "The following code runs the environment with 3 random agents and one agent with human input (use the arrow keys on your keyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = pommerman_ffa_v0()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "# Add 3 random agents\n",
    "agents = {}\n",
    "for agent_id in range(3):\n",
    "    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "\n",
    "# Add human agent\n",
    "on_key_press, on_key_release = a.utility.get_key_control(\"arrows\")\n",
    "agents[3] = a.agents.PlayerAgent(\n",
    "    config[\"agent\"](agent_id, config[\"game_type\"]),\n",
    "    a.utility.KEY_INPUT,\n",
    "    on_key_press=on_key_press,\n",
    "    on_key_release=on_key_release)\n",
    "    \n",
    "env.set_agents(list(agents.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cinjon/Code/playground/venv/lib/python3.6/site-packages/scipy/misc/pilutil.py:480: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Users/cinjon/Code/playground/venv/lib/python3.6/site-packages/scipy/misc/pilutil.py:483: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': <Result.Win: 0>, 'winner': [2]}\n"
     ]
    }
   ],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(obs)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "# Print the result\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent\n",
    "\n",
    "The following code uses Tensorforce to train a PPO agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0d5169662ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make sure you have tensorforce installed: pip install tensorforce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_gym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIGym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/playground/venv/lib/python3.6/site-packages/tensorforce/agents/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/playground/venv/lib/python3.6/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorForceError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_parameter_recorder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaParameterRecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/playground/venv/lib/python3.6/site-packages/tensorforce/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSessionLog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Make sure you have tensorforce installed: pip install tensorforce\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Proximal Policy Optimization agent\n",
    "agent = PPOAgent(\n",
    "    states_spec=dict(type='float', shape=(169,)),\n",
    "    actions_spec=dict(type='int', num_actions=6),\n",
    "    network_spec=[\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    batch_size=128,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(obs):\n",
    "    return obs[\"board\"].reshape(-1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAIGym' object has no attribute 'agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-d9769cff1fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTensorforceAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAIGym' object has no attribute 'agents'"
     ]
    }
   ],
   "source": [
    "class TensorforceAgent(a.agents.Agent):\n",
    "    def act(self, obs, action_space):\n",
    "        return agent.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = pommerman_ffa_v0()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "# Add 3 random agents\n",
    "agents = {}\n",
    "for agent_id in range(3):\n",
    "    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agents[3] = TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "env.set_agents(list(agents.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How do you wrap the environment to make it compatible with gym?\n",
    "class WrappedEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "    \n",
    "    def step(self, action):\n",
    "        actions = env.act(env._get_observations())\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        agent_obs = featurize(obs[3])\n",
    "        return agent_obs, reward[3], done, info\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._env.reset()\n",
    "        \n",
    "    def _render(self, mode='human', close=False, record_dir=None):\n",
    "        self._env.render(mode, close, record_dir)\n",
    "        \n",
    "    def _seed(self, seed=None):\n",
    "        self._env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env = WrappedEnv(env)\n",
    "runner = Runner(agent=agent, environment=wrapped_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run(episodes=1, max_episode_timesteps=200)\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
